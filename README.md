# Portfolio

## Adaboost Classifier with UCI Heart Disease Dataset
  As part of coursework for my Machine Learning class, we were tasked with writing an AdaBoost classifier from scratch using a hypothesis space of all possible constructions of a decision tree with splits on two attributes. This entailed writing an algorithm to construct the trees themselves, construction of a function to parse the constructed trees for predicted labels, and implementation of the AdaBoost algorithm based on theory presented in class. Not only did this project give an in-depth understanding of this algorithm, but it also provided valuable experience in debugging a program with this level of complexity.

## Game Agent Optimization Using the A* Algorithm
  This project as part of my Artificial Intelligence course asked for the creation of an agent that solves the 8-puzzle problem using a range of search techniques. Before implementing any of the search algorithms, I first had to design an implementation for the nodes and the construction of the tree to be searched, operationalizing the rules of the game to have the children of a given node represent the possible next game states based on the current state. Once this was done for the base tree and depth first search, the same methods were leveraged for the iterative deepening search- simply calling depth-first search starting with a depth of one and incrementing and rerunning until either the goal is found or the depth limit is reached. For the A* implementation, java’s object-oriented nature was leveraged to have a new A* node class inherit the original node class, only adding an additional attribute for the heuristic distance. Additionally, implementation of the comparable interface allows for straightforward integration of library data structures such as the priority queue. This project allowed for practice utilizing data structures in a manner that readily emphasizes their real-world utility, gave great practice in leveraging the object-oriented programming paradigm, and was a great exercise in returning to a larger program, re-understanding the mechanisms of the particular implementation, and making it more efficient and readable.

## Image Classification using Convolutional Neural Networks
	For the Computational Methods for Data Science course, one of the later projects we were tasked with was to leverage transfer learning to create a deep neural network that could classify images. The images themselves were not specified, so part of the learning experience for this project was finding a quality dataset, performing adequate preprocessing, and packaging the data in a manner that tensorflow/keras could work with. In the model construction and training portion, a wide range of layer combinations on top of the transfer model were tested, along with a range of values for each parameter for two different transfer models. Due to the restriction on compute performance, parameters had to be tuned manually rather than through a grid search, but this allowed my partner and I to gain a better understanding of which parameters had more of an impact on the model performance.

## Principal Component Analysis and Support Vector Machines with Slack
	Again for my Machine Learning course, we were tasked with using what we had previously learned regarding support vector machines with slack to explore the utility of principal component analysis. Given a dataset with 60 features, we were asked to first construct a classifier with varying degrees of allowable slack that projected the data down to a subspace of 1-6 dimensions in order to find the optimal values for the subspace and slack hyperparameters. We were then asked to modify the program to use the principal components found to construct a probability distribution giving the most probability density to the features that have the most importance in the principal components, essentially leveraging the technique to create a classifier with reduced dimensionality that has the benefit of being a more explainable model. The key takeaway from this project outside of the depth of understanding I received regarding the technique of PCA is insight into how to apply traditional techniques in novel ways to more thoroughly explore a given dataset- it highlights the importance of thinking deeply and creatively about these methods and considering the data, not just blindly applying them hoping to stumble upon the optimal solution.

## REMI- Recipe Exploration and Modification Intelligence
	This is an active project for my natural language processing course- a chatbot designed to assist users in the discovery of recipes and the modification of them if they are missing ingredients or uncomfortable with a given technique. As this is still in progress, the end functionality is still being determined, but currently this serving as a way for me to merge my passions for computer science and cooking, challenging myself to leverage as much as I possibly can from what I’ve learned throughout my degree to make it as capable as possible.
